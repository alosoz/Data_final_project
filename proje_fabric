ğŸ“Š Rapor: Booking Pipeline â€“ Microsoft Fabric
1. Veri KaynaÄŸÄ± â†’ Ingestion

AraÃ§: Eventstream (Fabric)

Ä°ÅŸ: Kaynak dosya, API veya streaming veri alÄ±mÄ±

Ã‡Ä±ktÄ±: Bronze Layer (OneLake iÃ§inde ham veri)

2. Bronze â†’ Silver (Temizleme & Transformasyon)

AraÃ§: Fabric Lakehouse Notebook (PySpark veya Pandas)

Ä°ÅŸ: Schema dÃ¼zenleme, null/duplicate temizliÄŸi, veri kalitesi kontrolÃ¼

Ã‡Ä±ktÄ±: Silver Layer (temizlenmiÅŸ tablo Lakehouse iÃ§inde)

3. Silver â†’ Gold (Modelleme & HazÄ±rlÄ±k)

AraÃ§:

Fabric Lakehouse (Delta tablolar ile modelleme)

Fabric Data Warehouse (SQL-based modelleme)

Ä°ÅŸ: Join, aggregation, business rules

Ã‡Ä±ktÄ±: Gold Layer (raporlamaya hazÄ±r business veri modeli)

4. Analiz & GÃ¶rselleÅŸtirme

AraÃ§: Power BI (Fabric entegre)

Ä°ÅŸ: Dashboard ve raporlarÄ±n hazÄ±rlanmasÄ± (Fabric iÃ§inden direkt eriÅŸim)

Ã‡Ä±ktÄ±: Self-service BI & raporlama

5. Orkestrasyon & Ä°zleme

AraÃ§:

Fabric Data Pipeline (ADF benzeri orchestrator)

Fabric Monitoring (pipeline + eventstream izleme)

âœ… Ã–zet KarÅŸÄ±laÅŸtÄ±rma

Azureâ€™da ingestion iÃ§in ADF kullanÄ±lÄ±rken Fabricâ€™te Eventstream Ã¶ne Ã§Ä±kÄ±yor.

Azureâ€™da transform iÃ§in Databricks kullanÄ±lÄ±rken Fabricâ€™te Lakehouse Notebook.

Azureâ€™da SQL katmanÄ± iÃ§in Synapse SQL Pool gerekirken Fabricâ€™te Data Warehouse gÃ¶mÃ¼lÃ¼.

Fabricâ€™in artÄ±sÄ±: tek platformda entegre, OneLake Ã¼zerinden tÃ¼m katmanlara eriÅŸim.






# Booking.com â€” Microsoft Fabric Data Engineering Pipeline

**DokÃ¼man:** AdÄ±m adÄ±m proje dosyasÄ± ve rehberi
**Hedef:** Ä°ki haftada tamamlanabilir, ML iÃ§ermeyen, Bronzeâ†’Silverâ†’Gold pipeline (Eventstream, Lakehouse, Notebooks, Dataflow Gen2, Warehouse, Power BI)

---

## Ä°Ã§indekiler

1. Ã–zet ve baÅŸarÄ± kriterleri
2. Ã–nkoÅŸullar (Fabric workspace, izinler)
3. Repo yapÄ±sÄ± (dosya listesi ve aÃ§Ä±klamalarÄ±)
4. AdÄ±m-adÄ±m uygulama (gÃ¼n gÃ¼n, her adÄ±mda oluÅŸturulacak dosyalar ve iÃ§erikleri)
5. Notebook / script ÅŸablonlarÄ± (kod bloklarÄ±)
6. Data pipeline ve Ã¶rnek aktiviteler
7. Warehouse & Power BI adÄ±mlarÄ±
8. Testler, kalite kontrolleri ve runbook
9. Teslim ve demo kontrol listesi

---

## 1) Ã–zet ve Kabul Kriterleri

* Bronze/Silver/Gold Delta tablolarÄ± Fabric Lakehouse Ã¼zerinde mevcut.
* Eventstream â†’ Lakehouse Bronze â†’ Notebook (Transforms) â†’ Silver/Gold Ã§alÄ±ÅŸÄ±r durumda.
* Warehouse ile en az 5 analytic sorgu Ã§alÄ±ÅŸtÄ±rÄ±labiliyor.
* Power BI raporu (PBIX) Ã§alÄ±ÅŸÄ±yor, 4 gÃ¶rsel iÃ§eriyor.
* Basit veri kalite testleri (unique booking\_id, mandatory null checks) geÃ§iyor.

---

## 2) Ã–nkoÅŸullar

* Microsoft Fabric workspace
* OneLake (otomatik olarak saÄŸlanÄ±r)
* Lakehouse (Ã¶rn: `booking_lakehouse`)
* Eventstream (stream veya batch ingestion iÃ§in)
* Dataflow Gen2 (temel transformlar iÃ§in opsiyonel)
* Power BI Desktop

EriÅŸim roller: Fabric Contributor.

---

## 3) Ã–nerilen Repo YapÄ±sÄ± (oluÅŸturacaÄŸÄ±n dosyalar)

```
repo-root/
  â”œâ”€ data/                            # kÃ¼Ã§Ã¼k Ã¶rnek CSV dosyalarÄ± (Ã¶ÄŸrenciler iÃ§in)
  â”‚    â”œâ”€ booking_sample.csv
  â”‚    â”œâ”€ reviews_sample.csv
  â”‚    â””â”€ cities.csv
  â”œâ”€ notebooks/
  â”‚    â”œâ”€ 01_bronze_ingest_notebook.ipynb
  â”‚    â”œâ”€ 02_stream_simulator.py
  â”‚    â”œâ”€ 03_bronze_to_silver.ipynb
  â”‚    â”œâ”€ 04_silver_to_gold.ipynb
  â”‚    â””â”€ 05_quality_checks.ipynb
  â”œâ”€ pipelines/
  â”‚    â””â”€ fabric_data_pipeline.json
  â”œâ”€ scripts/
  â”‚    â””â”€ deploy_instructions.md
  â”œâ”€ powerbi/
  â”‚    â””â”€ Booking_Dashboard_Template.pbix
  â”œâ”€ tests/
  â”‚    â””â”€ data_quality_checks.py
  â””â”€ docs/
       â””â”€ runbook.md
```

---

## 4) GÃ¼n-GÃ¼n Uygulama (AdÄ±m AdÄ±m)

### HazÄ±rlÄ±k (0.5 gÃ¼n)

* Fabric workspace aÃ§.
* Lakehouse oluÅŸtur.
* Repo'yu klonla; `data/` iÃ§ine Ã¶rnek CSV koy.

### GÃ¼n 1 â€” Veri & KeÅŸif

* Task: `data/` iÃ§indeki CSV'leri incele, `data/dictionary.md` oluÅŸtur.
* Ã‡Ä±ktÄ±: `data/booking_sample.csv` vs. sÃ¼tunlar.

### GÃ¼n 2 â€” Eventstream â†’ Bronze

* Task: Eventstream oluÅŸtur, input (CSV veya simÃ¼lasyon dosyasÄ±) â†’ output (Lakehouse/bronze).
* Dosya: `pipelines/fabric_data_pipeline.json` (Ã¶rnek export).

### GÃ¼n 3 â€” Bronze ingest notebook

* Task: `notebooks/01_bronze_ingest_notebook.ipynb` oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r.
* Bu notebook: bronze tablolarÄ± organize eder, ingestion\_ts ekler.

### GÃ¼n 4 â€” Bronze â†’ Silver dÃ¶nÃ¼ÅŸÃ¼mleri

* Task: `notebooks/03_bronze_to_silver.ipynb` Ã§alÄ±ÅŸtÄ±r.
* Ä°ÅŸler: tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼, tarih alanlarÄ±, dedupe, zorunlu alan kontrolleri.

### GÃ¼n 5 â€” Dim / Fact table tasarÄ±mÄ± ve Gold

* Task: `notebooks/04_silver_to_gold.ipynb`
* OluÅŸtur: `dim_hotel`, `dim_city`, `fact_booking` (MERGE Ã¶rneÄŸi)

### GÃ¼n 6 â€” Warehouse External Tables & SQL

* Task: Warehouse oluÅŸtur ve Lakehouse gold tablolarÄ±na baÄŸla.
* Ã‡alÄ±ÅŸtÄ±r: 5 temel sorgu (top city by revenue, cancellation rate, ADR trend)

### GÃ¼n 7 â€” Power BI Dashboard

* Task: `powerbi/Booking_Dashboard_Template.pbix` ile baÄŸlantÄ± ve rapor.

### GÃ¼n 8 â€” Orkestrasyon: Fabric Pipeline

* Task: Fabric pipeline'Ä± finalize et: Eventstream â†’ Bronze â†’ Notebook Silver â†’ Notebook Gold

### GÃ¼n 9 â€” Testler, Log ve Runbook

* Task: `notebooks/05_quality_checks.ipynb` Ã§alÄ±ÅŸtÄ±r; runbook.md yaz.

### GÃ¼n 10 â€” Final demo & teslim

* Task: Demo, Power BI canlÄ± gÃ¶sterim, repo sunumu.

---

## 5) Notebook / Script ÅablonlarÄ±

### `01_bronze_ingest_notebook.ipynb` (Ã¶zet)

```python
# Learning objectives: Lakehouse baÄŸlantÄ±sÄ±, CSV read, write to Delta (bronze)
from pyspark.sql.functions import current_timestamp

landing_path = 'Tables/landing/bookings/'
bronze_path = 'Tables/bronze/bookings/'

df = spark.read.option('header', True).csv(landing_path)
df2 = df.withColumn('ingestion_ts', current_timestamp())

df2.write.format('delta').mode('append').save(bronze_path)
```

### `02_stream_simulator.py` (opsiyonel)

*(AynÄ± mantÄ±k, sadece Eventstream input klasÃ¶rÃ¼ne JSON dosyalarÄ± yazacak ÅŸekilde ayarlanÄ±r.)*

### `03_bronze_to_silver.ipynb` (Ã¶zet)

```python
from pyspark.sql.functions import to_date, col

bronze = 'Tables/bronze/bookings/'
silver = 'Tables/silver/bookings/'

df = spark.read.format('delta').load(bronze)

clean = (df
  .withColumn('checkin_date', to_date(col('arrival_date'), 'yyyy-MM-dd'))
  .withColumn('checkout_date', to_date(col('departure_date'), 'yyyy-MM-dd'))
  .dropDuplicates(['booking_id'])
)

clean.write.format('delta').mode('overwrite').save(silver)
```

### `04_silver_to_gold.ipynb` (Ã¶zet)

```python
silver_bookings = 'Tables/silver/bookings/'
gold_fact = 'Tables/gold/fact_booking/'

bookings = spark.read.format('delta').load(silver_bookings)

# dim city
dim_city = bookings.select('city','country').distinct()
dim_city.write.format('delta').mode('overwrite').save('Tables/gold/dim_city/')

# fact booking
bookings.select('booking_id','hotel_id','city','checkin_date','checkout_date','adr','is_canceled').write.format('delta').mode('overwrite').save(gold_fact)
```

### `05_quality_checks.ipynb` (Ã¶zet)

```python
from pyspark.sql.functions import col

path = 'Tables/silver/bookings/'
df = spark.read.format('delta').load(path)

total = df.count()
distinct = df.select('booking_id').distinct().count()
assert total == distinct, f'Duplicates found: {total-distinct}'

null_count = df.filter(col('hotel_id').isNull() | col('checkin_date').isNull()).count()
assert null_count == 0, f'Nulls in mandatory fields: {null_count}'
```

---

## 6) Fabric Data Pipeline (Ã¶rnek akÄ±ÅŸ)

* Activity1: **Eventstream** â€” Input â†’ Lakehouse Bronze
* Activity2: **Notebook** â€” `01_bronze_ingest_notebook.ipynb`
* Activity3: **Notebook** â€” `03_bronze_to_silver.ipynb`
* Activity4: **Notebook** â€” `04_silver_to_gold.ipynb`

---

## 7) Warehouse & Power BI

* Warehouse: Lakehouse Gold tablolarÄ±na baÄŸlÄ± external tables.
* Power BI: Warehouse endpointâ€™e baÄŸlan, rapor sayfalarÄ± hazÄ±rlayÄ±n:

  * KPI (Total Bookings, Cancellation Rate, ADR)
  * Trend (bookings over time)
  * Map (city location)
  * Table (recent bookings)

---

## 8) Testler, Monitoring ve Runbook

* Test scriptleri `tests/data_quality_checks.py` iÃ§inde.
* Monitor: Fabric pipeline runs, Lakehouse tablolarÄ± kontrolÃ¼.
* Runbook.md iÃ§eriÄŸi:

  * HatalÄ± pipeline nasÄ±l yeniden Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
  * Common errors & fixes
  * Where to find logs

---

## 9) Teslim ve Demo Checklist

* [ ] Repo pushlandÄ± (README, data Ã¶rnekleri)
* [ ] Lakehouse iÃ§inde bronze/silver/gold yollar dolu
* [ ] Fabric pipeline baÅŸarÄ±lÄ± run
* [ ] Warehouse external table sorgulanabiliyor
* [ ] Power BI raporu Ã§alÄ±ÅŸÄ±yor
* [ ] Runbook.md hazÄ±r

---

**Son:** Bu yeni versiyonda istersen ilk olarak Eventstream ile `bronze` ingestion kÄ±smÄ±nÄ± mÄ±, yoksa Notebook Silver transformunu mu detaylÄ± aÃ§ayÄ±m?
