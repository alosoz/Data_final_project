ğŸ“Š Rapor: Booking Pipeline â€“ Azure Data Platform
1. Veri KaynaÄŸÄ± â†’ Ingestion

AraÃ§: Azure Data Factory (ADF) Pipeline

Ä°ÅŸ: Kaynak dosyalar (CSV/JSON) veya APIâ€™den veri alÄ±mÄ±

Ã‡Ä±ktÄ±: Bronze Layer (Data Lake Storage Gen2 â€“ ham veri)

2. Bronze â†’ Silver (Temizleme & Transformasyon)

AraÃ§: Azure Databricks (PySpark)

Ä°ÅŸ: Veri kalitesi kontrolleri, schema enforcement, null/duplicate temizliÄŸi

Ã‡Ä±ktÄ±: Silver Layer (temizlenmiÅŸ & analize hazÄ±r veri)

3. Silver â†’ Gold (Modelleme & HazÄ±rlÄ±k)

AraÃ§: Databricks veya Azure Synapse SQL Pools

Ä°ÅŸ: Join, aggregation, business rules uygulanmasÄ±

Ã‡Ä±ktÄ±: Gold Layer (business reporting tablosu)

4. Analiz & GÃ¶rselleÅŸtirme

AraÃ§: Power BI (Azure Synapse baÄŸlantÄ±sÄ± Ã¼zerinden)

Ä°ÅŸ: Dashboard ve raporlarÄ±n hazÄ±rlanmasÄ±

Ã‡Ä±ktÄ±: Self-service BI & raporlama

5. Orkestrasyon & Ä°zleme

AraÃ§:

ADF (pipeline scheduling)

Azure Monitor / Log Analytics (alert & izleme)




Ã–zet (hedef)

AmaÃ§: Azure araÃ§larÄ±yla uÃ§tan uca data engineering + analytics pipeline kurmak (Bronzeâ†’Silverâ†’Gold), Databricks ile dÃ¶nÃ¼ÅŸÃ¼mler, Synapse Serverless ile sorgu, Power BI ile dashboard.

KÄ±sÄ±tlama: Ä°leri ML yok. Proje 2 haftada (10 iÅŸ gÃ¼nÃ¼) tamamlanabilir olmalÄ±; karmaÅŸÄ±k streaming yerine opsiyonel basit stream-simÃ¼lasyonu (dosya tabanlÄ± veya Event Hubs ile basit producer) bÄ±rakÄ±yoruz.

Hedef kitle: Yeni Ã¶ÄŸrenen Data Engineer / Data Analyst Ã¶ÄŸrencilere uygun, hands-on.

Ã–nerilen sadeleÅŸtirmeler / dÃ¼zeltmeler (neden & nasÄ±l)

ML kÄ±smÄ±nÄ± Ã§Ä±kar

Neden: Ek sÃ¼re, veri mÃ¼hendisliÄŸi Ã¶ÄŸrenme hedefini daÄŸÄ±tÄ±r.

Ne yapÄ±lÄ±r yerine: Gold katmanda â€œfeatureâ€ ve Ã¶zet KPIâ€™lar Ã¼ret, Power BIâ€™da gÃ¶rselleÅŸtir.

Streamingâ€™i opsiyonel ve basitleÅŸtir

Orijinal karmaÅŸÄ±klÄ±k: Event Hubs + ASA veya Structured Streaming tam prod.

Ã–neri:

Minimum: SimÃ¼le edilmiÅŸ stream iÃ§in bir Python script ile kÃ¼Ã§Ã¼k JSON dosyalarÄ± landing/stream/ iÃ§ine drop et ve ADF/Databricks ile dÃ¼zenli (her 5 dk) pick-up yap.

Ä°steÄŸe baÄŸlÄ± geliÅŸmiÅŸ: GerÃ§ek Event Hubs + Databricks Structured Streaming olarak gÃ¶ster, ama materyal/Ã¶devde opsiyonel bÄ±rak.

ADF complexity azalt

TÃ¼m copy aktivitelerini ADFâ€™ye koy; fakat Databricks notebook tetiklemelerini Databricks Jobs veya ADFâ€™den bir â€œRunNotebookâ€ aktivitesi ile yap. BÃ¶ylece ADF yalnÄ±zca ingest ve orkestrasyondan sorumlu olur.

Infrastructure as Code (IaC) opsiyonel tut

EÄŸer sÃ¼re kÄ±sÄ±tÄ± varsa, IaC yerine adÄ±m adÄ±m portal veya minimal ARM/Bicep Ã¶rneÄŸi ver. Ä°leri dÃ¼zey olanlar iÃ§in repoâ€™da infrastructure/ ile bir Ã¶rnek bÄ±rak.

Great Expectations yerine hafif testler

Full GE kurulum zaman alÄ±r. Bunun yerine: Databricks Ã¼zerinde basit pytests veya PySpark assertions (Ã¶rn. null check, unique booking_id) kullan. Ä°stersen GE Ã¶rneÄŸi â€œekâ€ olarak ver.

Datasets: kÃ¼Ã§Ã¼k Ã¶rneklerle baÅŸla

Tam 500k+ veri yerine kÃ¼Ã§Ã¼k Ã¶rnek CSVâ€™ler (10kâ€“50k) kullan. Demo iÃ§in Ã¶rnekleri repo/data iÃ§ine koy veya Kaggleâ€™dan kÃ¼Ã§Ã¼k export al.

Dev ortamÄ±

Databricks Community veya Azure Databricks (dev) kullan. Synapse Serverless kullanÄ±mÄ± iÃ§in maliyet dÃ¼ÅŸÃ¼k ve kurulumu basit.

Minimal (Ã¶ÄŸrenci dostu) Teknoloji Listesi

Zorunlu (Ã¶ÄŸrenme hedefine uygun):

ADLS Gen2 (landing / bronze / silver / gold)

Azure Databricks (PySpark, Delta)

Azure Data Factory (orchestration, copy)

Azure Key Vault (connection secrets)

Azure Synapse Serverless (external table / SQL)

Power BI (dashboard)

Opsiyonel (ileri/isteÄŸe baÄŸlÄ±):

Azure Event Hubs (gerÃ§ek stream)

Databricks Structured Streaming (eÄŸer Event Hubs kullanÄ±lÄ±r)

Great Expectations (veri kalite kapsamÄ±nÄ± artÄ±rmak iÃ§in)

IaC (Bicep/Terraform) â€” tercih edilen ama opsiyonel

Yeni, sadeleÅŸtirilmiÅŸ Repo YapÄ±sÄ± (Ã¶ÄŸrenciler iÃ§in)
repo-root/
  â”œâ”€ data/                # kÃ¼Ã§Ã¼k sample CSV'ler (booking_sample.csv, reviews_sample.csv, cities.csv)
  â”œâ”€ notebooks/
  â”‚   â”œâ”€ 01_ingest_adf_instructions.md
  â”‚   â”œâ”€ 02_bronze_ingest_notebook.py
  â”‚   â”œâ”€ 03_bronze_to_silver.py
  â”‚   â”œâ”€ 04_silver_to_gold.py
  â”‚   â””â”€ 05_synapse_external_tables.sql
  â”œâ”€ pipelines/           # ADF JSON Ã¶rnekleri (kopya + run notebook)
  â”œâ”€ scripts/
  â”‚   â”œâ”€ stream_simulator.py   # opsiyonel - kÃ¼Ã§Ã¼k JSON producer (dosya drop)
  â”œâ”€ tests/
  â”‚   â””â”€ data_quality_checks.py
  â””â”€ docs/
      â””â”€ runbook.md

GÃ¼n GÃ¼n (2 hafta / 10 iÅŸ gÃ¼nÃ¼) Plan â€” Ã–ÄŸrenci Dostu (adÄ±m adÄ±m)

HazÄ±rlÄ±k (Ã¶ncesi): Azure aboneliÄŸi, resource group, Databricks workspace, ADLS gen2, KeyVault. (bu hazÄ±rlÄ±k 0.5 gÃ¼n olarak gÃ¶rÃ¼lÃ¼r)

1. GÃ¼n â€” Proje baÅŸlangÄ±Ã§ & veri keÅŸfi (4â€“6 saat)

Hedef: Veri kaynaklarÄ±nÄ± indir, Ã¶rnek CSVâ€™leri repo/data/ koy.

Ã‡Ä±ktÄ±: README + veri sÃ¶zlÃ¼ÄŸÃ¼ (hangi sÃ¼tun, tip).

2. GÃ¼n â€” ADLS & eriÅŸim + ADF copy (4â€“6 saat)

ADLS container oluÅŸtur: landing/ ve bronze/.

ADF ile local CSV â†’ ADLS landing/ kopya pipeline oluÅŸtur.

Ã‡Ä±ktÄ±: successful copy run.

3. GÃ¼n â€” Databricks ortam & basit bronze ingest notebook (4â€“6 saat)

Mount veya direkt credentials ile ADLSâ€™ye eriÅŸ.

Notebook 02_bronze_ingest_notebook.py: landing â†’ bronze (Delta) dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (binary ingest / autoinfer).

Ã‡Ä±ktÄ±: bronze delta path iÃ§inde veriler.

4. GÃ¼n â€” Bronze temizlik & silver dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (4â€“6 saat)

03_bronze_to_silver.py: tip dÃ¶nÃ¼ÅŸÃ¼mleri, zorunlu alan kontrolÃ¼, dedupe.

Basit veri kalite kontrolleri (null%<threshold, unique booking_id).

Ã‡Ä±ktÄ±: silver delta tablolar.

5. GÃ¼n â€” Dim/Facts & gold hazÄ±rlÄ±ÄŸÄ± (4â€“6 saat)

04_silver_to_gold.py : dim_hotel, dim_city, fact_booking oluÅŸtur.

MERGE INTO kullanarak incremental yazÄ±m gÃ¶ster (Ã¶rnek kÃ¼Ã§Ã¼k).

Ã‡Ä±ktÄ±: gold klasÃ¶rÃ¼ ve Ã¶rnek tablolar.

6. GÃ¼n â€” Synapse Serverless external tables + SQL views (4 saat)

05_synapse_external_tables.sql dosyasÄ± ile external table oluÅŸtur.

Basit analytic queries hazÄ±rla (top city by revenue, cancellation rate).

Ã‡Ä±ktÄ±: Synapse Ã¼zerinden Ã§alÄ±ÅŸtÄ±rÄ±labilir SQL.

7. GÃ¼n â€” Power BI dashboard & dataset (4â€“6 saat)

Power BI Desktop ile Synapse veri kaynaÄŸÄ±na baÄŸlan, 4-5 gÃ¶rsel (KPI, harita, iptal oranÄ±, ADR trend).

Ã‡Ä±ktÄ±: .pbix dosyasÄ± + kÄ±sa sunum.

8. GÃ¼n â€” Orkestrasyon (ADF) & scheduling (4 saat)

ADF pipeline: Copy(landing) â†’ Run Databricks(02) â†’ Run Databricks(03/04) â†’ Trigger synapse refresh (opsiyonel).

Test: pipeline Ã§alÄ±ÅŸtÄ±r, loglar, alert baseline.

9. GÃ¼n â€” Testler, kalite kontrolleri, runbook (4 saat)

Basit test scriptleri (data_quality_checks.py) Ã§alÄ±ÅŸtÄ±r, failure senaryolarÄ± test et.

Runbook.md: nasÄ±l debug yapÄ±lÄ±r, checkpoint/where to look.

10. GÃ¼n â€” Sunum & teslimat + buffer (4â€“6 saat)

Final demo: 10â€“15 dk sunum, Power BI canlÄ±, kÄ±sa repo tour.

Buffer: hata dÃ¼zeltmeleri, eksik dokÃ¼manlar.

Not: EÄŸer Event Hubs/Stream istersen, bunu 1 ekstra gÃ¼n (opsiyonel) ekle; yoksa stream_simulator ile dosya-drop simÃ¼le ederiz.

Kabul Kriterleri (BasitleÅŸtirilmiÅŸ ve Ã¶lÃ§Ã¼lebilir)

Bronze, Silver, Gold klasÃ¶rlerinde Delta tablolar var ve booking_id unique (KPI: unique% >= 99.9).

ADF pipeline baÅŸarÄ±lÄ± ÅŸekilde Ã§alÄ±ÅŸÄ±yor ve Databricks notebookâ€™larÄ±nÄ± tetikliyor.

Synapse Serverless ile en az 5 analytic sorgu Ã§alÄ±ÅŸtÄ±rÄ±labiliyor (Ã¶rnek sorgular hazÄ±r).

Power BI raporu Ã§alÄ±ÅŸÄ±yor ve en az 4 gÃ¶rsel iÃ§eriyor (KPI, trend, map, table).

Basit veri kalite testleri (null check, date range, unique id) geÃ§iyor.

TÃ¼m adÄ±mlar iÃ§in runbook/README bulunuyor.

Ã–rnek veri kalite kontrolleri (Databricks/PySpark)
# Basit kontroller Ã¶rneÄŸi
df = spark.read.format("delta").load("/mnt/datalake/booking_clean/bookings/")
# 1. booking_id unique
total = df.count()
distinct = df.select("booking_id").distinct().count()
assert distinct == total, f"Duplicate booking_id found: {total-distinct} duplicates"

# 2. zorunlu alanlar dolu
null_critical = df.filter(df["hotel_id"].isNull() | df["checkin_date"].isNull()).count()
assert null_critical == 0, f"Critical nulls: {null_critical}"

# 3. tarih aralÄ±ÄŸÄ± kontrolÃ¼
from pyspark.sql.functions import year
years = df.select(year("checkin_date")).distinct().collect()
print("Years present:", years)

Basit Ã¶rnek PySpark snippet â€” Bronze â†’ Silver (doÄŸru tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼ + dedupe)
from pyspark.sql.functions import to_date, col

bronze = "/mnt/datalake/booking_raw/bookings_delta/"
silver = "/mnt/datalake/booking_clean/bookings_delta/"

df = spark.read.format("delta").load(bronze)
df2 = (df
      .withColumn("checkin_date", to_date(col("arrival_date"), "yyyy-MM-dd"))
      .withColumn("checkout_date", to_date(col("departure_date"), "yyyy-MM-dd"))
      .withColumn("lead_time", col("lead_time").cast("int"))
      .dropDuplicates(["booking_id"])
)

df2.write.format("delta").mode("overwrite").save(silver)

DeÄŸerlendirme RubriÄŸi (Bootcamp uyarlanmÄ±ÅŸ)

Pipeline & Orkestrasyon: 30%

Veri TemizliÄŸi & Model (dim/fact): 25%

Analitik sorgular & Power BI dashboard: 25%

Test & DokÃ¼mantasyon (runbook): 20%

Ek Notlar / Ä°puÃ§larÄ± (pratik)

Cluster: Databricksâ€™de kÃ¼Ã§Ã¼k sÄ±nÄ±f cluster (2â€“4 worker) yeterli; auto-terminate ayarla.

Secrets: Key Vault kullan; notebookâ€™ta doÄŸrudan connection string yazma.

Delta optimize: OPTIMIZE ve VACUUM gÃ¶stermeye gerek yok, opsiyonel.

Veri boyutu: Demo 10kâ€“50k satÄ±r yeterli; bÃ¼yÃ¼k veri performansÄ±nÄ± istersen synthetic step ekle.

Ã–ÄŸretici materyal: Her notebook baÅŸÄ±na â€œlearning objectivesâ€ yaz â€” Ã¶ÄŸrencilere ne Ã¶ÄŸrenecekleri net olsun.

EÄŸer istersen bir sonraki adÄ±mda hemen ÅŸu dosyalarÄ± ben hazÄ±rlayayÄ±m (hemen bu konuÅŸmada):

02_bronze_ingest_notebook.py â€” hazÄ±r PySpark notebook (ADLS path parametreli)

03_bronze_to_silver.py â€” temizleme notebook (Ã¶zellikle tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼, dedupe, basit normalizasyon)

data/ iÃ§ine kÃ¼Ã§Ã¼k Ã¶rnek CSV (Ã¶rnek 5k satÄ±r) â€” fakat veri yÃ¼klemek iÃ§in benden veri indirmemi istersen bunu saÄŸlayamam; sana dosya ÅŸablonu hazÄ±rlayÄ±p nasÄ±l dolduracaÄŸÄ±n gÃ¶steririm.

Hangisini istersin hemen hazÄ±rlayayÄ±m? (not: eÄŸer â€œhemen hazÄ±rlaâ€ dersen, doÄŸrudan notebook kodlarÄ±nÄ± buraya eklerim).

Åunu dedin:
bana gore bana proje dosyasini adim adim bastan sona yunlendirecek sekilde hazirla
ChatGPT:
Booking Azure Data Pipeline Step By Step
Booking.com â€” Azure Data Engineering Pipeline

DokÃ¼man: AdÄ±m adÄ±m proje dosyasÄ± ve rehberi Hedef: Ä°ki haftada tamamlanabilir, ML iÃ§ermeyen, Bronzeâ†’Silverâ†’Gold pipeline (Databricks, ADLS, ADF, Synapse, Power BI)

Ä°Ã§indekiler

Ã–zet ve baÅŸarÄ± kriterleri

Ã–nkoÅŸullar (Azure kaynaklarÄ±, izinler)

Repo yapÄ±sÄ± (dosya listesi ve aÃ§Ä±klamalarÄ±)

AdÄ±m-adÄ±m uygulama (gÃ¼n gÃ¼n, her adÄ±mda oluÅŸturulacak dosyalar ve iÃ§erikleri)

Notebook / script ÅŸablonlarÄ± (kod bloklarÄ±)

ADF pipeline ve Ã¶rnek aktiviteler

Synapse & Power BI adÄ±mlarÄ±

Testler, kalite kontrolleri ve runbook

Teslim ve demo kontrol listesi

1) Ã–zet ve Kabul Kriterleri

Bronze/Silver/Gold Delta tablolarÄ± ADLS Ã¼zerinde mevcut.

ADF pipeline: Landing â†’ Bronze â†’ Databricks(Transforms) â†’ Gold Ã§alÄ±ÅŸÄ±r durumda.

Synapse Serverless ile en az 5 analytic sorgu Ã§alÄ±ÅŸtÄ±rÄ±labiliyor.

Power BI raporu (PBIX) Ã§alÄ±ÅŸÄ±yor, 4 gÃ¶rsel iÃ§eriyor.

Basit veri kalite testleri (unique booking_id, mandatory null checks) geÃ§iyor.

2) Ã–nkoÅŸullar

Azure subscription, Resource Group

ADLS Gen2 hesabÄ± (container: datalake)

Databricks workspace (dev cluster oluÅŸturulacak)

Azure Data Factory workspace

Azure Key Vault (secretler iÃ§in)

Power BI Desktop

EriÅŸim roller: Storage Blob Data Contributor (veya uygun mount/credential eriÅŸimi).

3) Ã–nerilen Repo YapÄ±sÄ± (oluÅŸturacaÄŸÄ±n dosyalar)
repo-root/
  â”œâ”€ data/                            # kÃ¼Ã§Ã¼k Ã¶rnek CSV dosyalarÄ± (Ã¶ÄŸrenciler iÃ§in)
  â”‚    â”œâ”€ booking_sample.csv
  â”‚    â”œâ”€ reviews_sample.csv
  â”‚    â””â”€ cities.csv
  â”œâ”€ notebooks/
  â”‚    â”œâ”€ 01_bronze_ingest_notebook.py
  â”‚    â”œâ”€ 02_stream_simulator.py
  â”‚    â”œâ”€ 03_bronze_to_silver.py
  â”‚    â”œâ”€ 04_silver_to_gold.py
  â”‚    â””â”€ 05_quality_checks.py
  â”œâ”€ pipelines/
  â”‚    â””â”€ adf_copy_and_run_notebook.json
  â”œâ”€ scripts/
  â”‚    â””â”€ deploy_instructions.md
  â”œâ”€ powerbi/
  â”‚    â””â”€ Booking_Dashboard_Template.pbix
  â”œâ”€ tests/
  â”‚    â””â”€ data_quality_checks.py
  â””â”€ docs/
       â””â”€ runbook.md

Her notebook baÅŸÄ±na kÄ±sa bir aÃ§Ä±klama satÄ±rÄ± (learning objectives) ekle.

4) GÃ¼n-GÃ¼n Uygulama (AdÄ±m AdÄ±m)
HazÄ±rlÄ±k (0.5 gÃ¼n)

Azure kaynaklarÄ± oluÅŸtur: Resource Group, Storage (ADLS Gen2), Databricks workspace, ADF, Key Vault.

KÄ±sa not: Key Vault iÃ§ine storage account key veya service principal secret kaydet.

Repo'yu klonla; data/ iÃ§ine Ã¶rnek CSV koy.

GÃ¼n 1 â€” Veri & KeÅŸif

Task: data/ iÃ§indeki CSV'leri incele, data/dictionary.md oluÅŸtur.

Ã‡Ä±ktÄ±: data/booking_sample.csv vs. sÃ¼tunlar.

GÃ¼n 2 â€” ADLS Landing + ADF Copy

Task: ADF Linked Service (ADLS), Dataset (CSV), Copy activity ile localâ†’landing/booking/ yÃ¼kle.

Dosya: pipelines/adf_copy_and_run_notebook.json (Ã¶rnek pipeline export).

GÃ¼n 3 â€” Databricks baÄŸlantÄ±sÄ± & Bronze ingest notebook

Task: notebooks/01_bronze_ingest_notebook.py oluÅŸtur ve Ã§alÄ±ÅŸtÄ±r.

Bu notebook: landing/ iÃ§indeki CSV'leri okur, basit schema apply eder, Delta formatÄ±nda bronze/bookings/ yaz.

GÃ¼n 4 â€” Bronze -> Silver dÃ¶nÃ¼ÅŸÃ¼mleri

Task: notebooks/03_bronze_to_silver.py Ã§alÄ±ÅŸtÄ±r.

Ä°ÅŸler: tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼, checkin_date/checkout_date to_date, dedupe, zorunlu alan kontrolleri.

GÃ¼n 5 â€” Dim / Fact table tasarÄ±mÄ± ve Gold

Task: notebooks/04_silver_to_gold.py

OluÅŸtur: dim_hotel, dim_city, fact_booking (MERGE Ã¶rneÄŸi)

GÃ¼n 6 â€” Synapse External Tables & SQL

Task: docs/synapse_instructions.md ekle: Serverless external data source, external table tanÄ±mlarÄ±

Ã‡alÄ±ÅŸtÄ±r: 5 temel sorgu (top city by revenue, cancellation rate, ADR trend)

GÃ¼n 7 â€” Power BI Dashboard

Task: powerbi/Booking_Dashboard_Template.pbix ile baÄŸlantÄ± ve rapor.

GÃ¼n 8 â€” Orkestrasyon: ADF Pipeline

Task: ADF pipeline'Ä± finalize et: Copy â†’ Run Databricks(01) â†’ Run Databricks(03) â†’ Run Databricks(04)

GÃ¼n 9 â€” Testler, Log ve Runbook

Task: notebooks/05_quality_checks.py Ã§alÄ±ÅŸtÄ±r; runbook.md yaz.

GÃ¼n 10 â€” Final demo & teslim

Task: Demo, Power BI canlÄ± gÃ¶sterim, repo sunumu.

5) Notebook / Script ÅablonlarÄ±

NOT: AÅŸaÄŸÄ±daki kod bloklarÄ± notebook iÃ§ine konulacak (parametre ile ADLS path alarak). Her notebook baÅŸÄ±na Learning objectives satÄ±rÄ± ekle.

01_bronze_ingest_notebook.py (Ã¶zet)
# Learning objectives: ADLS baÄŸlantÄ±sÄ±, CSV read, write to Delta (bronze)
from pyspark.sql.functions import current_timestamp


landing_path = '/mnt/datalake/landing/bookings/'
bronze_path = '/mnt/datalake/booking_raw/bookings_delta/'


df = spark.read.option('header', True).csv(landing_path)
# Basit transform: ingestion_ts
df2 = df.withColumn('ingestion_ts', current_timestamp())


(df2.write.format('delta').mode('append').save(bronze_path))
02_stream_simulator.py (opsiyonel)
# Basit dosya-drop simulator: booking_sample.csv'den rastgele satÄ±rlar alÄ±p json olarak landing/stream/ dizinine koyar
import csv, json, random, uuid, datetime, time


src = 'data/booking_sample.csv'
dst_folder = '/path-to-mounted/landing/stream/'


with open(src) as f:
    reader = list(csv.DictReader(f))


while True:
    rec = random.choice(reader)
    rec['event_id'] = str(uuid.uuid4())
    rec['ts'] = datetime.datetime.utcnow().isoformat()
    fname = dst_folder + f'event_{uuid.uuid4().hex}.json'
    with open(fname,'w') as out:
        out.write(json.dumps(rec))
    time.sleep(2)
03_bronze_to_silver.py (Ã¶zet)
# Learning objectives: data cleaning, type cast, dedupe
from pyspark.sql.functions import to_date, col


bronze = '/mnt/datalake/booking_raw/bookings_delta/'
silver = '/mnt/datalake/booking_clean/bookings_delta/'


df = spark.read.format('delta').load(bronze)


clean = (df
  .withColumn('checkin_date', to_date(col('arrival_date'), 'yyyy-MM-dd'))
  .withColumn('checkout_date', to_date(col('departure_date'), 'yyyy-MM-dd'))
  .withColumn('lead_time', col('lead_time').cast('int'))
  .dropDuplicates(['booking_id'])
)


clean.write.format('delta').mode('overwrite').save(silver)
04_silver_to_gold.py (Ã¶zet)
# Learning objectives: dim/fact creation, MERGE incremental
silver_bookings = '/mnt/datalake/booking_clean/bookings_delta/'
gold_fact = '/mnt/datalake/booking_gold/fact_booking/'


# Ã¶rnek: dim_city oluÅŸtur
bookings = spark.read.format('delta').load(silver_bookings)


# dim city (basit)
dim_city = bookings.select('city','country','city_lat','city_lon').distinct()
dim_city.write.format('delta').mode('overwrite').save('/mnt/datalake/booking_gold/dim_city/')


# fact booking (Ã¶rnek)
bookings.select('booking_id','hotel_id','city','checkin_date','checkout_date','adr','is_canceled').write.format('delta').mode('overwrite').save(gold_fact)
05_quality_checks.py (Ã¶zet)
# Simple assertions
from pyspark.sql.functions import col


path = '/mnt/datalake/booking_clean/bookings_delta/'
df = spark.read.format('delta').load(path)


# 1) booking_id unique
total = df.count()
distinct = df.select('booking_id').distinct().count()
assert total == distinct, f'Duplicates found: {total-distinct}'


# 2) mandatory fields not null
null_count = df.filter(col('hotel_id').isNull() | col('checkin_date').isNull()).count()
assert null_count == 0, f'Nulls in mandatory fields: {null_count}'
6) ADF Pipeline (Ã¶rnek akÄ±ÅŸ)

Activity1: Copy â€” Local/Blob â†’ ADLS landing/

Activity2: Databricks Notebook â€” 01_bronze_ingest_notebook.py

Activity3: Databricks Notebook â€” 03_bronze_to_silver.py

Activity4: Databricks Notebook â€” 04_silver_to_gold.py

ADF'de her notebook aktivitesi iÃ§in Databricks linked service ve notebook path parametrelerini kullan.

pipelines/adf_copy_and_run_notebook.json dosyasÄ±nÄ± ADF export formatÄ±nda koy; Ã¶rnek adÄ±mlar iÃ§ersin.

7) Synapse & Power BI

Synapse: Serverless external data source tanÄ±mla (ADLS URL). External table ile gold parquet/delta eriÅŸimi.

Power BI: Synapse endpoint'e baÄŸlan, rapor sayfalarÄ± hazÄ±rlayÄ±n:

KPI (Total Bookings, Cancellation Rate, ADR)

Trend (bookings over time)

Map (city location)

Table (recent bookings)

8) Testler, Monitoring ve Runbook

Test scriptleri tests/data_quality_checks.py iÃ§inde.

Monitor: ADF runs, Databricks job runs, storage checkpoint klasÃ¶rleri kontrolÃ¼.

Runbook.md iÃ§eriÄŸi:

HatalÄ± pipeline nasÄ±l yeniden Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r

Common errors & fixes

Where to find logs

9) Teslim ve Demo Checklist




Ek kaynaklar & Ã¶neriler

Databricks docs: Delta Lake, MERGE INTO examples

ADF docs: copy activity, databricks notebook activity

Son: Bu dokÃ¼manÄ± kullanarak hangi dosyayÄ± ilk oluÅŸturayÄ±m? (Ã¶r. 01_bronze_ingest_notebook.py tam kodunu canvas iÃ§inde yazayÄ±m veya pipelines/adf_copy_and_run_notebook.json Ã¶rneÄŸini oluÅŸturayÄ±m.)

DokÃ¼manÄ± oluÅŸturdum: Booking Azure Data Pipeline Step By Step. AÃ§Ä±p adÄ±mlarÄ±, dosya ÅŸablonlarÄ±nÄ± ve gÃ¼nlÃ¼k planÄ± oradan takip edebilirsin. Hangi dosyayÄ± hemen iÃ§ine tam olarak yazmamÄ± istersin? (Ã¶r. 01_bronze_ingest_notebook.py veya ADF pipeline JSON Ã¶rneÄŸi)
