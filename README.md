# Data_final_project

# Proje: Booking.com Esinli Azure Data Engineering Pipeline

## 1) Proje AmacÄ±

GerÃ§ek dÃ¼nyaya yakÄ±n bir senaryo ile, otel rezervasyon ve yorum verilerini (batch) destinasyon/hava durumu gibi zenginleÅŸtirme verileri ve gerÃ§ek zamanlÄ± olay akÄ±ÅŸÄ± (stream) ile birleÅŸtirerek **Bronzeâ€“Silverâ€“Gold (Delta Lake)** mimarisinde iÅŸlemeyi; **Azure Databricks** Ã¼zerinde dÃ¶nÃ¼ÅŸÃ¼mler yapmayÄ±; **Azure Synapse**â€™te analitik/sorgu katmanÄ±nÄ± kurmayÄ±; **Azure ML** ile basit bir tahmin modeli (Ã¶rn. iptal olasÄ±lÄ±ÄŸÄ± veya sonraki destinasyon) eÄŸitmeyi; **Power BI** ile son kullanÄ±cÄ± panosu hazÄ±rlamayÄ± hedefler.

BaÅŸarÄ± Ã¶lÃ§Ã¼tÃ¼: UÃ§tan uca pipelineâ€™Ä±n Ã§alÄ±ÅŸÄ±r olmasÄ±, zamanlanabilir/izlenebilir olmasÄ±, veri doÄŸrulama testlerini geÃ§mesi ve BI/ML Ã§Ä±ktÄ±larÄ±nÄ±n Ã¼retilebilmesi.

---

## 2) Kapsam ve Veri Setleri

* **Batch (ana veri):** Hotel Booking Demand (Kaggle) + TripAdvisor Hotel Reviews (Ã¶rnek yorumlar)
* **ZenginleÅŸtirme:** World Cities (ÅŸehirâ€“Ã¼lkeâ€“koordinat), statik hava durumu Ã¶zetleri veya OpenWeatherMapâ€™ten periyodik snapshot, kur bilgileri (opsiyonel)
* **GerÃ§ek ZamanlÄ± (stream):** Rezervasyon olaylarÄ±nÄ±n simÃ¼lasyonu (check-in/out event, fiyat gÃ¼ncellemesi vb.)

> Not: TÃ¼m batch dosyalarÄ± Ã¶nce **ADLS Gen2 / landing** klasÃ¶rÃ¼ne yÃ¼klenir.

---

## 3) Hedef Mimari (Ã–zet)

```
Data Sources (Kaggle CSV, APIs)        Real-time Simulator (Python)
        |                                         |
        v                                         v
   Azure Data Factory (Copy)                Azure Event Hubs
        |                                         |
        v                                         v
     ADLS Gen2 (Bronze - Raw)  <â€”â€” Azure Stream Analytics / Databricks Structured Streaming â€”â€”>
        |
        v
   Databricks (PySpark, Delta)
        |  (ETL/ELT)
        v
     ADLS Gen2 (Silver - Cleaned)
        |
        v
    Business Logic / Aggregations (Databricks)
        |
        v
     ADLS Gen2 (Gold - Curated)
        |                           
        +â€”â€”> Synapse Serverless / Dedicated SQL Pools â€”â€”> Power BI
        |
        +â€”â€”> Batch/Real-time Scores to Gold/Synapse
```

---

## 4) KullanÄ±lacak Azure Servisleri ve GerekÃ§eleri

* **ADLS Gen2 (Data Lake):** KatmanlÄ± veri depolama (Bronze/Silver/Gold), Delta Lake desteÄŸi
* **Azure Databricks:** PySpark ile dÃ¶nÃ¼ÅŸÃ¼m, Delta Lake, MLflow entegrasyonu
* **Azure Data Factory (veya Synapse Pipelines):** Batch ingest, orkestrasyon, zamanlama
* **Azure Event Hubs:** GerÃ§ek zamanlÄ± olay akÄ±ÅŸÄ± giriÅŸi
* **Azure Stream Analytics** (opsiyonel) **veya Databricks Structured Streaming:** Streamâ€™i Bronzeâ€™a yazma ve basit zenginleÅŸtirme
* **Azure Synapse Analytics:** Gold veriler iÃ§in SQL Query ve Semantic Layer
* **Power BI:** Raporlama ve dashboard
* **Azure Monitor / Log Analytics:** Ä°zleme, alarmlar
* **Key Vault:** Gizli anahtarlar ve baÄŸlantÄ± stringleri

---

## 5) Ortam ve Ã–nkoÅŸullar

* Azure Subscription, Resource Group, VNet (opsiyonel)
* Workspaceâ€™ler: Databricks, Synapse, AML
* RBAC: Storage Blob Data Contributor/Reader rolleri, least-privilege prensibi
* Naming & path standardÄ± (Ã¶rnek):

  * `adls://datalake/booking_raw/` (bronze), `.../booking_clean/` (silver), `.../booking_gold/` (gold)
  * `schema.dataset/version=1.0/ingestion_date=YYYY-MM-DD/`

---

## 6) Repo YapÄ±sÄ± (Ã–neri)

```
repo-root/
  â”œâ”€ infrastructure/ (Bicep/Terraform IaC)
  â”œâ”€ data/ (Ã¶rnek CSV, kÃ¼Ã§Ã¼k sampleâ€™lar)
  â”œâ”€ notebooks/
  â”‚   â”œâ”€ 01_bronze_ingest_batch.py
  â”‚   â”œâ”€ 02_stream_to_bronze.py
  â”‚   â”œâ”€ 03_silver_transformations.py
  â”‚   â”œâ”€ 04_gold_business_marts.py
  â”œâ”€ pipelines/ (ADF JSON tanÄ±mlarÄ±)
  â”œâ”€ powerbi/ (pbix veya dataset tanÄ±mlarÄ±)
  â”œâ”€ tests/ (great_expectations, pytest)
  â””â”€ docs/ (readme, runbooks)
```

---

## 7) UÃ§tan Uca AdÄ±mlar

### 7.1 Batch Ingestion (ADF)

1. ADFâ€™de **Linked Service**: ADLS, GitHub (opsiyonel), Key Vault
2. **Datasets**: CSV/Parquet tanÄ±mÄ± (Hotel Booking, Reviews, Cities, Weather snapshots)
3. **Copy Activities** ile `landing â†’ bronze` taÅŸÄ±ma, schema drift iÃ§in **Binary** ingest + Auto-Detect veya Mapping
4. **Validation**: Dosya boyutu, satÄ±r sayÄ±sÄ±, schema check (Great Expectations veya Databricks expectationâ€™larÄ±)

### 7.2 Stream Ingestion (Event Hubs â†’ Bronze)

* Python simÃ¼latÃ¶r, rezervasyon olaylarÄ±nÄ± JSON olarak Event Hubsâ€™a gÃ¶nderir (Ã¶r. `booking_id, user_id, hotel_id, event_type, ts`)
* **SeÃ§enek A:** Stream Analytics job: EH â†’ ADLS (Avro/JSON) + tumbling window ile kÃ¼Ã§Ã¼k agregasyonlar
* **SeÃ§enek B:** Databricks Structured Streaming: EH connector â†’ Delta Bronze

### 7.3 Bronze â†’ Silver (Databricks)

* Temizlik: Nullâ€™lar, tip dÃ¶nÃ¼ÅŸtÃ¼rme, tarih/Ã¼lke normalizasyonu
* Join: reviews + hotels + cities + weather snapshot
* Dedupe, slowly changing attributes (SCD Type 2 gerekli ise)

### 7.4 Silver â†’ Gold (Databricks)

* Ä°ÅŸ kurallarÄ±: KPIâ€™lar (doluluk, avg\_daily\_rate, cancellation\_rate, lead\_time buckets)
* Boyut-kÃ¼p yapÄ±larÄ±: `dim_hotel`, `dim_city`, `dim_date`, `fact_booking`
* Ä°nkremental yazÄ±m (MERGE INTO, Z-order gibi Delta optimizasyonlarÄ±)

### 7.5 Synapse KatmanÄ±

* Gold klasÃ¶rÃ¼ndeki Delta/Parquet tablolarÄ±na **Serverless SQL** ile external table veya Dedicated SQL Poolâ€™a yÃ¼kleme
* View/Stored Procedure ile semantic layer

### 7.6 Power BI

* Kaynak: Synapse SQL endpoint veya Gold Parquet/Delta
* Sayfalar: YÃ¶netim Ã–zeti, Operasyon (iptal & doluluk), Fiyat & Gelir, CoÄŸrafi daÄŸÄ±lÄ±m, ML skor daÄŸÄ±lÄ±mÄ±

### 7.9 Orkestrasyon ve Zamanlama

* ADF Pipeline:

  * Activity sÄ±rasÄ±: **Copy (batch)** â†’ **Databricks Notebook (bronzeâ†’silver)** â†’ **Databricks (silverâ†’gold)** â†’ **Synapse SQL scripts** â†’ **Power BI dataset refresh**
* Trigger: GÃ¼nlÃ¼k (batch), saatlik (stream append kontrol), manuel tetikleme

### 7.9 Ä°zleme & Kalite

* **Great Expectations** veya **Databricks Expectations**: satÄ±r sayÄ±sÄ±, benzersiz anahtarlar, tarih aralÄ±klarÄ±
* ADF ve Databricks jobâ€™larÄ± iÃ§in uyarÄ±lar (Azure Monitor, Action Groups)
* Maliyet izleme: Cost Analysis, tagâ€™ler

---

## 8) Ã–rnek Åžema ve Alanlar

**`fact_booking` (Gold)**

* booking\_id (PK), hotel\_id (FK), city\_id (FK), checkin\_date, checkout\_date
* adr (average daily rate), adults, children, babies
* is\_canceled, lead\_time, market\_segment, distribution\_channel
* review\_sentiment\_score (joined), weather\_avg\_temp, currency\_rate

**`dim_hotel`**: hotel\_id, hotel\_name, stars, chain\_flag, city\_id

**`dim_city`**: city\_id, city\_name, country, lat, lon, region

**`dim_date`**: date\_key, day, week, month, quarter, year, season

---

## 9) Kod Ã–rnekleri (Ã–zet)

### 9.1 Event Hubs â€“ Python SimÃ¼latÃ¶r (Ãœretici)

```python
import json, time, uuid, random, datetime
from azure.eventhub import EventHubProducerClient, EventData

conn_str = "<EVENT_HUB_CONN>"
hub_name = "booking-events"
producer = EventHubProducerClient.from_connection_string(conn_str, eventhub_name=hub_name)

hotels = [101,102,103,104]
users = list(range(1000,1100))
events = ["search", "view", "book", "cancel"]

while True:
    event = {
        "event_id": str(uuid.uuid4()),
        "booking_id": random.randint(1, 10_000),
        "user_id": random.choice(users),
        "hotel_id": random.choice(hotels),
        "event_type": random.choices(events, weights=[0.4,0.3,0.2,0.1])[0],
        "ts": datetime.datetime.utcnow().isoformat()
    }
    with producer:
        batch = producer.create_batch()
        batch.add(EventData(json.dumps(event)))
        producer.send_batch(batch)
    time.sleep(0.3)
```

### 9.2 Databricks â€“ Structured Streaming (EH â†’ Bronze Delta)

```python
from pyspark.sql.types import *
from pyspark.sql.functions import *

connectionString = "<EH_CONN>"
conf = {
  "eventhubs.connectionString": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)
}
raw = (
  spark.readStream.format("eventhubs")
  .options(**conf)
  .load()
)
json_df = raw.select(col("body").cast("string").alias("json")).select(from_json("json", "event_id string, booking_id long, user_id long, hotel_id long, event_type string, ts timestamp").alias("data")).select("data.*")

bronze_path = "abfss://datalake@<account>.dfs.core.windows.net/booking_raw/events/"
(
  json_df
  .withColumn("ingestion_date", to_date(current_timestamp()))
  .writeStream.format("delta")
  .option("checkpointLocation", bronze_path+"_chk")
  .option("path", bronze_path)
  .outputMode("append")
  .start()
)
```

### 9.3 Databricks â€“ Bronze â†’ Silver DÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Batch)

```python
from pyspark.sql.functions import *

bronze_bookings = "/mnt/datalake/booking_raw/bookings/"
silver_bookings = "/mnt/datalake/booking_clean/bookings/"

(df := spark.read.format("delta").load(bronze_bookings)) \
  .withColumn("checkin_date", to_date("arrival_date")) \
  .withColumn("lead_time", col("lead_time").cast("int")) \
  .dropDuplicates(["booking_id"]) \
  .write.format("delta").mode("overwrite").save(silver_bookings)
```

### 9.4 Databricks â€“ Silver â†’ Gold (MERGE INTO)

```sql
CREATE TABLE IF NOT EXISTS gold.fact_booking USING DELTA LOCATION '/mnt/datalake/booking_gold/fact_booking';

MERGE INTO gold.fact_booking AS tgt
USING silver.vw_fact_booking AS src
ON tgt.booking_id = src.booking_id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
```

### 9.5 Synapse â€“ External Table (Serverless)

```sql
CREATE EXTERNAL DATA SOURCE ds_dlake WITH (
  LOCATION = 'https://<account>.dfs.core.windows.net/datalake'
);
CREATE EXTERNAL FILE FORMAT ff_parquet WITH (FORMAT_TYPE = PARQUET);
CREATE EXTERNAL TABLE dbo.fact_booking
WITH (
  LOCATION = '/booking_gold/fact_booking/',
  DATA_SOURCE = ds_dlake,
  FILE_FORMAT = ff_parquet
)
AS SELECT * FROM OPENROWSET(
  BULK 'https://<account>.dfs.core.windows.net/datalake/booking_gold/fact_booking/',
  DATA_SOURCE = ds_dlake,
  FORMAT='PARQUET'
) AS rows;
```

## 10) Test, DoÄŸrulama ve Kabul Kriterleri

* **Veri Kalitesi:** Zorunlu alanlarÄ±n doluluÄŸu, tarih aralÄ±ÄŸÄ± kontrolleri, benzersiz `booking_id`
* **Ä°zlenebilirlik:** ADF/Databricks jobâ€™larÄ±nÄ±n baÅŸarÄ±lÄ± koÅŸularÄ±, checkpoint dosyalarÄ±, SLA raporu
* **Performans:** Batch ingest < X dakika, gÃ¼nlÃ¼k 1M satÄ±rda silver dÃ¶nÃ¼ÅŸÃ¼mleri < Y dakika
* **GÃ¼venlik:** EriÅŸimler RBAC ile kÄ±sÄ±tlÄ±, secretâ€™lar Key Vaultâ€™ta

---

## 11) DeÄŸerlendirme RubriÄŸi (Bootcamp)

* Mimari tasarÄ±m (20%) â€“ KatmanlÄ± yapÄ±, servis entegrasyonu
* Veri ingest ve dÃ¶nÃ¼ÅŸÃ¼mler (25%) â€“ ADF + Databricks kalitesi
* Veri modeli ve Gold tabakasÄ± (15%) â€“ Boyut-fakt yapÄ±sÄ±, performans
* Dashboard (15%) â€“ KPIâ€™lar, kullanÄ±labilirlik
* Ä°zleme ve kalite (10%) â€“ Testler, alarmlar

---

## 12) Zaman PlanÄ± (Ã–neri â€“ 5 GÃ¼n)

* **GÃ¼n 1:** Ortam kurulumu, veri kaynaklarÄ±, batch ingest
* **GÃ¼n 2:** Bronzeâ†’Silver dÃ¶nÃ¼ÅŸÃ¼mleri, veri kalitesi
* **GÃ¼n 3:** Silverâ†’Gold, Synapse external tables, SQL gÃ¶rÃ¼nÃ¼mleri
* **GÃ¼n 4:** Streaming hat, Event Hubs + Structured Streaming/ASA

(Alternatif: 2â€“3 gÃ¼nlÃ¼k yoÄŸunlaÅŸtÄ±rÄ±lmÄ±ÅŸ sÃ¼rÃ¼m; ML veya streaming kÄ±sÄ±tlÄ± tutulur.)

---

## 13) GeliÅŸmiÅŸ/Ä°steÄŸe BaÄŸlÄ±

* **dbt** ile transformlar, **Unity Catalog** ile yÃ¶netiÅŸim
* **CI/CD:** GitHub Actions + ADF/Databricks deployment
* **Feature Store** entegrasyonu, **Model Registry** (MLflow)
* **KullanÄ±m maliyeti optimizasyonu:** Auto-stop clusters, spot nodeâ€™lar, Delta optimize/z-order

---

## 14) Beklenen Ã‡Ä±ktÄ±lar

* Ã‡alÄ±ÅŸÄ±r **ADF pipeline** (JSON), Databricks notebookâ€™larÄ±, AML deneyi
* ADLSâ€™te **Bronze/Silver/Gold** Delta tablolarÄ±
* **Synapse** sorgulanabilir katman (external/dedicated)
* **Power BI** raporu (.pbix veya workspace dataset)
* **Runbook/README**: Kurulum ve Ã§alÄ±ÅŸtÄ±rma adÄ±mlarÄ±

---

## 15) Son Notlar

* Veri kaynaklarÄ± lisanslarÄ±na uyun; scraping yapacaksanÄ±z robots.txt ve kullanÄ±m koÅŸullarÄ±nÄ± kontrol edin.
* Demo amaÃ§lÄ± veri hacmini kÃ¼Ã§Ã¼k tutup, performans testleri iÃ§in Ã¶rnek bÃ¼yÃ¼tÃ¼lmÃ¼ÅŸ veri oluÅŸturabilirsiniz.
* Bootcamp sonunda ekipler bulgularÄ±nÄ± ve mimari kararlarÄ±nÄ± 10 dakikalÄ±k bir sunumla paylaÅŸsÄ±n.

---



âœ… 1. Kaggle

Booking.com veya otel rezervasyonuna benzer birÃ§ok dataset var:

Hotel Booking Demand Dataset

Ä°Ã§erik: Otel rezervasyonlarÄ±, tarih, iptal, konaklama sÃ¼resi, kiÅŸi sayÄ±sÄ± vb.

Hotel Reviews Data in Europe

Ä°Ã§erik: 515.000 otel yorumu, puanlar, otel Ã¶zellikleri.

Expedia Hotel Recommendations

Ä°Ã§erik: KullanÄ±cÄ± arama verileri, otel bilgileri.

Booking.com Hotel Reviews

Ä°Ã§erik: KullanÄ±cÄ± yorumlarÄ± ve puanlar.

âœ… 2. Open Data PortallarÄ±

DataHub.io

Google Dataset Search

European Data Portal

âœ… 3. GitHub Repoâ€™larÄ±

Awesome Public Datasets
 â†’ Turizm ve otel rezervasyon verileri mevcut.

BazÄ± projelerde direkt Booking.com verisi temizlenmiÅŸ ÅŸekilde var.

âœ… 4. Alternatif Kaynaklar

OpenWeather API â†’ Hava durumu verisi (destinasyon bazlÄ±)

City Population Data â†’ World Population Review
 veya Kaggle

Tourism Indicators â†’ UNWTO veya Kaggle

ðŸ’¡ Ä°stersen bu Kaggle veri setlerini seÃ§ip Azure Blob Storageâ€™a yÃ¼kleyerek projenin batch ingestion kÄ±smÄ±nÄ± yapabiliriz. GerÃ§ek zamanlÄ± simÃ¼lasyon iÃ§in de Event Hubâ€™a bu verilerden random veri gÃ¶nderecek bir Python script yazarÄ±z.

âœ… Sonraki adÄ±m:
Ä°stersen, ben senin iÃ§in bu Kaggle datasetlerini seÃ§eyim ve projede hangi katmanda nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± netleÅŸtireyim.
â†’ Hangi tÃ¼r veriler olsun?

Otel rezervasyon detaylarÄ± (booking_id, tarih, check-in, check-out, kiÅŸi sayÄ±sÄ±)

Otel yorumlarÄ± (review, rating, sentiment analizi)

Destinasyon/hava durumu

Fiyat ve mÃ¼saitlik bilgisi

Hangilerini dahil edelim? Yoksa hepsini mi?
